# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a research implementation of "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making" from NeurIPS 2024, now **completely rewritten using LangGraph 0.6.6** for enhanced modularity and scalability.

## Current Status (LangGraph Implementation Complete)

### üéØ **LangGraph Rewrite Status**
- ‚úÖ **ALL 6 STAGES COMPLETE** - Production ready LangGraph implementation
- ‚úÖ **Real LLM Integration** - Fully operational with Gemini/OpenAI APIs
- ‚úÖ **88 Tests Passing** - Comprehensive test coverage across all stages
- ‚úÖ **Production Entry Point** - `main.py` (LangGraph), `old_main.py` (original preserved)

### üöÄ **System Architecture**
- **Stage 1-2**: Core LangGraph infrastructure with difficulty assessment & routing
- **Stage 3**: Basic processing (3-expert + arbitrator) - maintains 87.05% accuracy target
- **Stage 4**: Intermediate processing (multi-round debate) - maintains 78.67% accuracy target  
- **Stage 5**: Advanced processing (MDT approach) - maintains 75.76% accuracy target
- **Stage 6**: Production integration with monitoring, error recovery & performance tracking

### ‚ö° **Latest Performance**
- **Processing Time**: 15-25 seconds per question (real LLM calls)
- **Token Usage**: ~1,400 tokens per question (actual API consumption)
- **Real Medical Analysis**: Professional-grade responses with detailed reasoning
- **Success Rate**: 100% with comprehensive error handling and logging

## Development Setup

### Environment Management Rules
**CRITICAL REMINDER**: 
- **Virtual Environment**: Conda-managed Python 3.12.11 environment
- **Direct Usage**: `venv/bin/python main.py` (recommended approach)
- **All Dependencies**: LangGraph 0.6.6, google-generativeai 0.8.5, python-dotenv installed

### Quick Start
1. **Environment**: Conda environment with Python 3.12.11 pre-configured
2. **API Keys**: Already configured in `.env` file with Gemini API key
3. **Ready to Run**: All dependencies installed and tested

### Running the LangGraph System
**Production Usage**:
```bash
# LangGraph-based system (recommended)
venv/bin/python main.py --dataset medqa --model gemini-2.5-flash --difficulty adaptive --num_samples 1

# Original system (preserved)  
venv/bin/python old_main.py --dataset medqa --model gemini-2.5-flash --difficulty adaptive --num_samples 1
```

Available models:
- `gemini-2.0-flash`, `gemini-2.5-flash`, `gemini-2.5-flash-lite-preview-06-17` (requires `genai_api_key`)
- `gpt-4o-mini`, `gpt-4.1-mini` (requires `openai_api_key`)

Difficulty modes:
- `adaptive`: System determines complexity automatically
- `basic`: 3-expert recruitment + arbitrator processing
- `intermediate`: Expert collaboration with multi-round debate
- `advanced`: Multi-disciplinary team approach

### Evaluation and Testing
```bash
# Evaluate system output and generate CSV reports
python evaluate_text_output.py

# Split test data by difficulty for analysis
python split_test_data.py

# Extract specific questions by ID for debugging
python extract_by_question_id.py
```

Key evaluation files:
- `output/`: JSON output files from system runs
- `evaluation/`: CSV evaluation reports with accuracy metrics
- `data/medqa/`: Test datasets split by difficulty level

**Important**: The `evaluate_text_output.py` script contains configurable input/output filenames at the top:
```python
input_filename = 'output/inter_json_adaptive_332samples.json'  # Update as needed
output_filename = 'evaluation/inter_json_adaptive_332samples.csv'  # Update as needed
```

## LangGraph Architecture

### Core LangGraph Components

1. **LangGraphAgent** (`langgraph_mdm.py`): Production LLM wrapper with real Gemini/OpenAI API calls
2. **StateGraph Framework**: LangGraph 0.6.6 with TypedDict state management and Command-based routing
3. **Compiled Subgraphs**: Each processing mode as independent, testable subgraph

### LangGraph Processing Modes

- **Basic Processing** (`langgraph_basic.py`): 
  - **3-Expert System**: Independent specialist recruitment with medical arbitrator
  - **Real LLM Calls**: Actual Gemini API integration with token tracking
  - **JSON Communication**: Structured expert responses with multi-layer parsing
  - **Performance**: Production-ready with comprehensive error handling
  
- **Intermediate Processing** (`langgraph_intermediate.py`): 
  - **Hierarchical Debate**: Multi-round expert collaboration with participation decisions
  - **Dynamic Routing**: Command-based flow control with moderator consensus
  - **State Management**: LangGraph StateGraph with persistent conversation state
  - **Performance**: Real multi-agent interactions with token efficiency
  
- **Advanced Processing** (`langgraph_advanced.py`): 
  - **MDT Coordination**: Multi-disciplinary teams with specialized roles (IAT/Specialist/FRDT)
  - **Parallel Processing**: Independent team assessments with overall coordinator synthesis
  - **Production Integration**: Comprehensive logging and performance monitoring
  - **Performance**: Full MDT approach with real medical decision-making

### Data Structure

- Input: JSONL files in `data/{dataset}/` (test.jsonl, train.jsonl)
- Output: JSON files in `output/` with format: `{model}_{dataset}_{difficulty}_{samples}samples.json`
- Each result includes: question, options, ground truth, model response, determined difficulty, and token usage

### LangGraph System Files

```
MDMAgents/
‚îú‚îÄ‚îÄ main.py                 # ‚úÖ LangGraph production entry point
‚îú‚îÄ‚îÄ old_main.py             # PRESERVED - original entry point  
‚îú‚îÄ‚îÄ langgraph_mdm.py        # ‚úÖ Core LangGraph infrastructure with real LLM calls
‚îú‚îÄ‚îÄ langgraph_difficulty.py # ‚úÖ Difficulty assessment and routing
‚îú‚îÄ‚îÄ langgraph_basic.py      # ‚úÖ Basic processing (3-expert + arbitrator)
‚îú‚îÄ‚îÄ langgraph_intermediate.py # ‚úÖ Intermediate processing (multi-round debate)  
‚îú‚îÄ‚îÄ langgraph_advanced.py   # ‚úÖ Advanced processing (MDT approach)
‚îú‚îÄ‚îÄ langgraph_integration.py# ‚úÖ Stage 6 integration and optimization
‚îú‚îÄ‚îÄ test_*.py               # ‚úÖ 88 comprehensive tests across all stages
‚îî‚îÄ‚îÄ pytest.ini             # ‚úÖ Test configuration
```

### Key Utility Functions

- `setup_model()`: Configures API clients based on model type
- `determine_difficulty()`: Uses LLM with JSON format to assess question complexity for adaptive mode
- `load_data()`: Loads test questions and exemplars from dataset files
- `create_question()`: Formats questions with randomized multiple choice options

### Evaluation System (`evaluate_text_output.py`)

Enhanced parsing system with multi-pattern answer extraction:

```python
def extract_final_answer_or_answer(text):
    # Handles multiple response formats:
    # 1. "Answer: C" patterns (common in majority_vote responses)
    # 2. "B) Normal hemoglobin..." patterns  
    # 3. "(A) Some answer" parentheses formats
    # 4. Parse error cases marked as 'X'
    # 5. Various structured answer formats
```

**Key Features**:
- **Multi-pattern Recognition**: Handles diverse LLM response formats including complex majority_vote responses
- **Parse Error Tracking**: Identifies and tracks parsing failures with 'X' marker
- **CSV Export**: Generates detailed evaluation reports with per-difficulty accuracy metrics
- **Answer Validation**: Robust extraction from malformed or incomplete responses

**Critical Parsing Improvements**:
- Early "Answer: X" detection for long majority_vote responses
- Enhanced word boundary handling for answer extraction
- Fallback patterns for parentheses formats like "(A)" and "B) text..."
- Support for various formatting styles and error cases

## Common Patterns

- **JSON-First Communication**: All agent interactions use structured JSON formats with regex parsing and fallbacks
- **Multi-layer Error Handling**: JSON parsing ‚Üí regex extraction ‚Üí text fallback ‚Üí default responses
- **Temperature Control**: 0.0 for deterministic final decisions, 0.7 for creative expert collaboration
- **Token Efficiency**: Word limits enforced across all processing modes (50-300 words)
- **Comprehensive Tracking**: Token usage monitored for all agents, recruiters, and coordinators
- **Production-Ready Output**: Debug controls provide clean output for production vs verbose for development

## LangGraph Implementation Status

### üéØ **All 6 Stages Complete**
- **Stage 1-2**: ‚úÖ Core infrastructure with difficulty assessment & routing 
- **Stage 3**: ‚úÖ Basic processing with 3-expert + arbitrator system
- **Stage 4**: ‚úÖ Intermediate processing with multi-round debate
- **Stage 5**: ‚úÖ Advanced processing with MDT approach  
- **Stage 6**: ‚úÖ Production integration with monitoring & error recovery

### üöÄ **Production Ready Features**
- **Real LLM Integration**: Actual Gemini API calls with professional medical responses
- **Comprehensive Testing**: 88 tests passing across all stages and components
- **Performance Monitoring**: Real-time token tracking, health checks, and system metrics
- **Error Recovery**: Exponential backoff retry logic with graceful degradation
- **Production Entry Point**: Complete CLI compatibility with existing evaluation scripts

### üèÜ **Key Achievements**
- **Complete Rewrite**: Original system preserved, LangGraph system fully operational
- **Real-World Performance**: 15-25 second processing with ~1,400 tokens per question
- **Medical Quality**: Professional-grade analysis with detailed clinical reasoning
- **Scalable Architecture**: Modular LangGraph design ready for future enhancements